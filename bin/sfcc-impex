#!/usr/bin/env bash
set -e
[ -n "$DEBUG" ] && set -x

#
# download files from Impex folder
#
# USAGE
#
#    sfcc impex [options] [path] [filename-pattern]
#
# OPTIONS
#
#    -d          download
#    -c N        limit number of file to N
#    -p          plot day histogram
#    -P DATE     plot hour histogram for DATE (default: today)
#
# EXAMPLES
#
#    sfcc impex -d log import-catalog-2019
#    sfcc impex archive/catalog
#    sfcc impex -P
#    sfcc impex -p "2019-05-03"
#

basepath="/on/demandware.servlet/webdav/Sites/Impex/"
host=$SFCC_DL_HOST
user=$SFCC_DL_USER
folder=""


histogram() {
  local max_value=`cat $1 | sort -n | tail -1 | awk '{print $1}'`
  local label_width=$2
  local num_cols="$(( `tput cols` - label_width - `echo -n $max_value | wc -c` - 10 ))"
  local step="$(( $max_value / $num_cols + 1 ))"
  if [ $step == 0 ]; then step=1; fi

  cat $datafile \
    | awk -v S="$step" -v Y=`tput setaf 3` -v R=`tput sgr0` -v G=`tput setaf 0` '{printf("\n  %s %s", $2, Y); for(i=0; i<$1; i+=S){printf("#")}; if ($1 > 0) {printf(" %s(%s)", G, $1)}; printf("%s", R)}' \
    | sort -n
}

daily_plot() {
  local datafile=`mktemp ./daily-plot-XXXXX`
  trap "rm $datafile" EXIT
  ls | grep -e "\.xml$" | perl -pe 's/^(\d{4}-\d{2}-\d{2}).*/\1/g' | sort | uniq -c > $datafile
  histogram $datafile 10
}

hourly_plot() {
  echo -e "\n$1:"
  local datafile=`mktemp ./hourly-plot-XXXXX`
  local datafile2=`mktemp ./hourly-plot-XXXXX-2`
  trap "rm $datafile $datafile2" EXIT
  ls | grep -e "\.xml$" | grep "^$1" | perl -pe 's/^'${1}'T(\d{2}).*/\1/g' | sort | uniq -c > $datafile2

  # fill empty hours
  for i in {00..23}; do
    n=`awk '/'${i}'$/ {print $2}' $datafile2`
    cn=`awk '/'${i}'$/ {print $0}' $datafile2`
    if [ "$i" == "$n" ]; then
      echo $cn
    else
      echo 0 $i
    fi
  done > $datafile
  histogram $datafile 2
}



while test $# -ne 0; do
  arg=$1
  shift
  case $arg in
    -h|--help)
      cat $0 | sed -n '/^#/p' | sed '/^##/d' | sed 1d | sed 's/#/ /g' \
        | perl -pe "s/Impex/$(tput setaf 2)${SFCC_DL_HOST}$(tput sgr0) Impex/" \
        | perl -pe "s/(USAGE|OPTIONS|EXAMPLES)/$(tput setaf 0)\1$(tput sgr0)/"
      exit 1
      ;;
    -P|--day-histogram)
      daily_plot
      exit 0
      ;;
    -p|--hour-histogram)
      hourly_plot ${1:-`date +"%Y-%m-%d"`}
      exit 0
      ;;
    -d|--download)
    download=1
      ;;
    -c|--count)
      count=$1
      shift
      ;;
    *)
      if [ $# -eq 1 ]; then
        folder=$arg
        arg=$1
        shift
        filepattern=$arg
      else
        folder=$arg
      fi
      ;;
  esac
done

urlfile=`mktemp ./url-XXXXX`
trap "rm $urlfile" EXIT

curl -sN -u "$user" "https://${host}${basepath}${folder}" \
  | sed -n '/.*<a href="\(.*\)"><tt>.*/p' \
  | sed 's/.*<a href="\(.*\)"><tt>.*/\1/g' \
  | ( [[ $filepattern ]] && sed -n /$filepattern/p || cat ) \
  | ( [[ $count ]] && head -n $count || cat ) \
  > $urlfile

if [ $download ]; then
  total=`wc -l $urlfile | awk '{print $1}'`
  i=0
  while ((i++)); read -r url; do
    file="`echo $url | sed 's/.*\/\(.*\)/\1/g'`"
    echo "$(tput setaf 3)[$i/$total]$(tput sgr0) $file"
    if [ ! -f $file ]; then
      curl -s -u "$user" -O "https://${host}${url}"
      if [ ${file: -4} == ".xml" ]; then
        xml fo $file > temp && mv temp $file || echo $file >> fo-error.txt
      fi
    fi
  done < $urlfile
else
  if [ -z $folder ]; then
    cat $urlfile | perl -pe 's/(Impex\/)(.*)$/\1'$(tput setaf 2)'\2'$(tput sgr0)'/g'
  else
    cat $urlfile | perl -pe 's/(Impex\/'${folder//\//\\/}'\/)(.*)$/\1'$(tput setaf 2)'\2'$(tput sgr0)'/g'
  fi
fi

exit 0

## TODO:
## 1. make multiple curl downloads in parallel (to speed up overall download time)
## 2. sometimes a downloaded xml file returns a html-transmission-error page. Theses files should not be saved.
## 3. add flag for direct download (if I know the exact file, I don't need to download the whole file list)
## 4. add option to set instance
## 5. add flag -a for zipping & archiving old files
